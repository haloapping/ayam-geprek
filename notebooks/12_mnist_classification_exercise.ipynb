{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a31fce7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T13:21:29.107550Z",
     "start_time": "2022-05-21T13:21:29.098085Z"
    }
   },
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dbc8f1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:14:42.020805Z",
     "start_time": "2022-05-22T13:14:31.992323Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import Lambda\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics import F1Score, ConfusionMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2705a7",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0fdf303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:14:42.211899Z",
     "start_time": "2022-05-22T13:14:42.033175Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\"../datasets/\", train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(\"../datasets/\", train=False, download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075cc304",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b17f5ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:14:42.243407Z",
     "start_time": "2022-05-22T13:14:42.223897Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77239a4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T12:39:49.891581Z",
     "start_time": "2022-05-21T12:39:49.866732Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1fb2a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T12:28:38.981717Z",
     "start_time": "2022-05-21T12:28:38.963817Z"
    }
   },
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8edc5f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:14:42.329644Z",
     "start_time": "2022-05-22T13:14:42.264583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=784, out_features=64, bias=True)\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=10, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (output): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_feature):\n",
    "        super(Model, self).__init__()\n",
    "                \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(n_feature, 64),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(64, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x_fc1)\n",
    "        output = self.output(x_fc2)\n",
    "        \n",
    "        return output\n",
    "\n",
    "model = Model(n_feature=28 * 28)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778f1ed",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "158b944c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:14:42.354740Z",
     "start_time": "2022-05-22T13:14:42.337569Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NLLLoss()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_metric = nn.NLLLoss()\n",
    "loss_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7308ee",
   "metadata": {},
   "source": [
    "## Eval Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "018682ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:14:42.384992Z",
     "start_time": "2022-05-22T13:14:42.360742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F1Score()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metric = F1Score(num_classes=10)\n",
    "eval_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c90028",
   "metadata": {},
   "source": [
    "## Optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "924f59b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:14:42.416039Z",
     "start_time": "2022-05-22T13:14:42.389983Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f6572d",
   "metadata": {},
   "source": [
    "## Loop Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b64c116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:20:49.988281Z",
     "start_time": "2022-05-22T13:20:02.175500Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/1\n",
      "==============================================\n",
      "Train\n",
      "\n",
      "Batch-100  | F1-Score: 0.875000 | Loss: -0.877855\n",
      "Batch-200  | F1-Score: 0.906250 | Loss: -0.887813\n",
      "Batch-300  | F1-Score: 0.812500 | Loss: -0.811459\n",
      "Batch-400  | F1-Score: 0.875000 | Loss: -0.879328\n",
      "Batch-500  | F1-Score: 0.875000 | Loss: -0.878619\n",
      "Batch-600  | F1-Score: 0.812500 | Loss: -0.813038\n",
      "Batch-700  | F1-Score: 0.906250 | Loss: -0.888269\n",
      "Batch-800  | F1-Score: 0.937500 | Loss: -0.942672\n",
      "Batch-900  | F1-Score: 0.812500 | Loss: -0.826092\n",
      "Batch-1000 | F1-Score: 0.843750 | Loss: -0.846943\n",
      "Batch-1100 | F1-Score: 0.906250 | Loss: -0.917132\n",
      "Batch-1200 | F1-Score: 0.843750 | Loss: -0.857841\n",
      "Batch-1300 | F1-Score: 0.812500 | Loss: -0.812059\n",
      "Batch-1400 | F1-Score: 0.843750 | Loss: -0.845949\n",
      "Batch-1500 | F1-Score: 0.843750 | Loss: -0.853192\n",
      "Batch-1600 | F1-Score: 0.906250 | Loss: -0.915142\n",
      "Batch-1700 | F1-Score: 0.937500 | Loss: -0.936639\n",
      "Batch-1800 | F1-Score: 0.906250 | Loss: -0.914866\n",
      "Accuracy : 0.8651833534240723\n",
      "Loss     : -0.8688795831044515\n",
      "\n",
      "Test\n",
      "\n",
      "Batch-100  | F1-Score: 0.875000 | Loss: -0.883565\n",
      "Batch-200  | F1-Score: 0.781250 | Loss: -0.789233\n",
      "Batch-300  | F1-Score: 0.875000 | Loss: -0.874106\n",
      "Accuracy : 0.8665000200271606\n",
      "Loss     : -0.8700704822144188\n",
      "==============================================\n",
      "\n",
      "SUMMARY:\n",
      "Train Accuracy : 0.8651833534240723\n",
      "Train Loss     : -0.8688795831044515\n",
      "\n",
      "Test Accuracy  : 0.8665000200271606\n",
      "Test Loss      : -0.8700704822144188\n",
      "Training time: 0.796 minutes\n"
     ]
    }
   ],
   "source": [
    "def train(dataloader, model, loss_metric, eval_metric, optimizer, time):\n",
    "    with open(\"../callbacks/\" + str(time) + \"/logs/history.txt\", \"a\") as f:\n",
    "        f.write(\"Train\\n\")\n",
    "        \n",
    "    print(\"Train\\n\")\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    for batch, (feature, actual_label) in enumerate(dataloader, 1):\n",
    "        # Forward Propagation\n",
    "        pred_label = model(feature)\n",
    "        loss = loss_metric(pred_label, actual_label)\n",
    "        evaluate_metric = eval_metric(pred_label, actual_label)\n",
    "        \n",
    "        # Backward Propagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss = loss.item()\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Batch-{batch:<4} | F1-Score: {evaluate_metric:>7f} | Loss: {loss:>7f}\")\n",
    "        \n",
    "            with open(\"../callbacks/\" + str(time) + \"/logs/history.txt\", \"a\") as f:\n",
    "                f.write(f\"Batch-{batch:<4} | F1-Score: {evaluate_metric:>7f} | Loss: {loss:>7f}\\n\")\n",
    "            \n",
    "    evaluate_metric = eval_metric.compute()\n",
    "    loss = sum(losses) / len(dataloader)\n",
    "    print(f\"Accuracy : {evaluate_metric}\")\n",
    "    print(f\"Loss     : {loss}\\n\")\n",
    "    \n",
    "    with open(\"../callbacks/\" + str(time) + \"/logs/history.txt\", \"a\") as f:\n",
    "        f.write(f\"Accuracy : {evaluate_metric}\\n\")\n",
    "        f.write(f\"Loss     : {loss}\\n\\n\")\n",
    "        \n",
    "    eval_metric.reset()\n",
    "    \n",
    "    return evaluate_metric, loss\n",
    "\n",
    "def validate(dataloader, model, loss_metric, eval_metric, time):\n",
    "    with open(\"../callbacks/\" + str(time) + \"/logs/history.txt\", \"a\") as f:\n",
    "        f.write(\"Test\\n\")\n",
    "        \n",
    "    print(\"Test\\n\")\n",
    "    \n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for batch, (feature, actual_label) in enumerate(dataloader, 1):\n",
    "        # Forward Propagation\n",
    "        pred_label = model(feature)\n",
    "        loss = loss_metric(pred_label, actual_label)\n",
    "        evaluate_metric = eval_metric(pred_label, actual_label)\n",
    "        \n",
    "        \n",
    "        loss = loss.item()\n",
    "        losses.append(loss)\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Batch-{batch:<4} | F1-Score: {evaluate_metric:>7f} | Loss: {loss:>7f}\")\n",
    "        \n",
    "            with open(\"../callbacks/\" + str(time) + \"/logs/history.txt\", \"a\") as f:\n",
    "                f.write(f\"Batch-{batch:<4} | F1-Score: {evaluate_metric:>7f} | Loss: {loss:>7f}\\n\")\n",
    "            \n",
    "    evaluate_metric = eval_metric.compute()\n",
    "    loss = sum(losses) / len(dataloader)\n",
    "    print(f\"Accuracy : {evaluate_metric}\")\n",
    "    print(f\"Loss     : {loss}\")\n",
    "    \n",
    "    with open(\"../callbacks/\" + str(time) + \"/logs/history.txt\", \"a\") as f:\n",
    "        f.write(f\"Accuracy : {evaluate_metric}\\n\")\n",
    "        f.write(f\"Loss     : {loss}\\n\\n\")\n",
    "        \n",
    "    eval_metric.reset()\n",
    "    \n",
    "    return evaluate_metric, loss\n",
    "\n",
    "def loop(train, validate, epochs=5):\n",
    "    now = datetime.now()\n",
    "    now = now.strftime(\"%m-%d-%Y_%H-%M-%S\")\n",
    "    os.makedirs(\"../callbacks/\" + str(now) + \"/models\")\n",
    "    os.makedirs(\"../callbacks/\" + str(now) + \"/logs\")\n",
    "\n",
    "    train_accuracies, train_losses = [], []\n",
    "    test_accuracies, test_losses = [], []\n",
    "\n",
    "    start = time()\n",
    "    for epoch in range(epochs):\n",
    "        with open(\"../callbacks/\" + str(now) + \"/logs/history.txt\", \"a\") as f:\n",
    "            f.write(f\"EPOCH {epoch + 1}/{epochs}\\n\")\n",
    "            f.write(f\"{'=' * 46}\\n\")\n",
    "            \n",
    "        print(f\"EPOCH {epoch + 1}/{epochs}\")\n",
    "        print(\"=\" * 46, end=\"\\n\")\n",
    "        acc_train, loss_train = train(train_loader, model, loss_metric, eval_metric, optimizer, now)\n",
    "        acc_test, loss_test = validate(test_loader, model, loss_metric, eval_metric, now)\n",
    "            \n",
    "        print(\"=\" * 46, end=\"\\n\\n\")\n",
    "        \n",
    "        with open(\"../callbacks/\" + str(now) + \"/logs/history.txt\", \"a\") as f:\n",
    "            f.write(f\"{'=' * 46}\\n\\n\")\n",
    "            \n",
    "        train_accuracies.append(acc_train)\n",
    "        train_losses.append(loss_train)\n",
    "        test_accuracies.append(acc_test)\n",
    "        test_losses.append(loss_test)\n",
    "        \n",
    "        torch.save(model.state_dict(), \"../callbacks/\" + now + \"/models/epoch_\" + str(epoch + 1) + \".pt\")\n",
    "    \n",
    "    metrics = {\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"train_losses\": train_losses,\n",
    "        \"test_accuracies\": test_accuracies,\n",
    "        \"test_losses\": test_losses\n",
    "    }\n",
    "    \n",
    "    torch.save(metrics, \"../callbacks/\" + now + \"/logs/metrics.pt\")\n",
    "\n",
    "    print(\"SUMMARY:\")\n",
    "    print(f\"Train Accuracy : {sum(train_accuracies) / len(train_accuracies)}\")\n",
    "    print(f\"Train Loss     : {sum(train_losses) / len(train_losses)}\\n\")\n",
    "    print(f\"Test Accuracy  : {sum(test_accuracies) / len(test_accuracies)}\")\n",
    "    print(f\"Test Loss      : {sum(test_losses) / len(test_losses)}\")\n",
    "    \n",
    "    with open(\"../callbacks/\" + now + \"/logs/history.txt\", \"a\") as f:\n",
    "        f.write(\"SUMMARY:\\n\")\n",
    "        f.write(f\"Train Accuracy : {sum(train_accuracies) / len(train_accuracies)}\\n\")\n",
    "        f.write(f\"Train Loss     : {sum(train_losses) / len(train_losses)}\\n\\n\")\n",
    "        f.write(f\"Test Accuracy  : {sum(test_accuracies) / len(test_accuracies)}\\n\")\n",
    "        f.write(f\"Test Loss      : {sum(test_losses) / len(test_losses)}\\n\")\n",
    "\n",
    "    stop = time()\n",
    "\n",
    "    total_time = stop - start\n",
    "    print(f\"Training time: {(total_time / 60):.3f} minutes\")\n",
    "    \n",
    "    with open(\"../callbacks/\" + now + \"/logs/history.txt\", \"a\") as f:\n",
    "        f.write(f\"Training time: {(total_time / 60):.3f} minutes\")\n",
    "        \n",
    "    return {\"train_accuracies\": train_accuracies, \"train_losses\": train_losses, \"test_accuracies\": test_accuracies, \"test_losses\": test_losses}\n",
    "\n",
    "summary = loop(train, validate, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f139b33f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:18:51.762698Z",
     "start_time": "2022-05-22T13:18:51.762698Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2, ncols=2, tight_layout=True, figsize=(12, 8))\n",
    "\n",
    "ax[0][0].set_title(\"Train\")\n",
    "ax[0][0].plot(summary[\"train_accuracies\"], label=\"accuracy\")\n",
    "ax[0][0].plot(summary[\"train_losses\"], label=\"loss\")\n",
    "ax[0][0].set_xlabel(\"Epoch\")\n",
    "ax[0][0].set_ylabel(\"Score\")\n",
    "ax[0][0].legend()\n",
    "ax[0][0].grid();\n",
    "\n",
    "ax[0][1].set_title(\"Test\")\n",
    "ax[0][1].plot(summary[\"test_accuracies\"], label=\"accuracy\")\n",
    "ax[0][1].plot(summary[\"test_losses\"], label=\"loss\")\n",
    "ax[0][1].set_xlabel(\"Epoch\")\n",
    "ax[0][1].set_ylabel(\"Score\")\n",
    "ax[0][1].legend()\n",
    "ax[0][1].grid();\n",
    "\n",
    "ax[1][0].set_title(\"Train Accuracy and Test Accuracy\")\n",
    "ax[1][0].plot(summary[\"train_accuracies\"], label=\"accuracy train\")\n",
    "ax[1][0].plot(summary[\"test_accuracies\"], label=\"accuracy test\")\n",
    "ax[1][0].set_xlabel(\"Epoch\")\n",
    "ax[1][0].set_ylabel(\"Score\")\n",
    "ax[1][0].legend()\n",
    "ax[1][0].grid();\n",
    "\n",
    "ax[1][1].set_title(\"Train Loss and Test Loss\")\n",
    "ax[1][1].plot(summary[\"train_losses\"], label=\"loss train\")\n",
    "ax[1][1].plot(summary[\"test_losses\"], label=\"loss test\")\n",
    "ax[1][1].set_xlabel(\"Epoch\")\n",
    "ax[1][1].set_ylabel(\"Score\")\n",
    "ax[1][1].legend()\n",
    "ax[1][1].grid();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3363f",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4e709",
   "metadata": {},
   "source": [
    "## Predict with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88c9aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:18:51.772465Z",
     "start_time": "2022-05-22T13:18:51.772465Z"
    }
   },
   "outputs": [],
   "source": [
    "features, labels = next(iter(test_loader))\n",
    "\n",
    "model.eval()\n",
    "pred_labels = model(features).argmax(dim=1)\n",
    "\n",
    "fig, axes = plt.subplots(8, 4, figsize=(24, 24))\n",
    "\n",
    "for image, label, pred_label, ax in zip(features, labels, pred_labels, axes.flatten()):\n",
    "    ax.imshow(image.permute(1, 2, 0).cpu(), cmap=plt.cm.gray)\n",
    "    font = {\"color\": \"red\"} if label != pred_label else {\"color\": \"green\"}\n",
    "    ax.set_title(f\"Label: {label} | Pred: {pred_label}\", fontdict=font)\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312b8a6",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe068df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:18:51.782350Z",
     "start_time": "2022-05-22T13:18:51.782350Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, dataloader):\n",
    "    actual_labels = dataloader.dataset.targets\n",
    "    features = dataloader.dataset.data.to(dtype=torch.float32)\n",
    "    \n",
    "    model.eval()\n",
    "    pred_labels = model(features).argmax(dim=1)\n",
    "    confusion_matrix = ConfusionMatrix(num_classes=10)\n",
    "    \n",
    "    ax = sns.heatmap(confusion_matrix(pred_labels, actual_labels), annot=True, fmt=\"d\")\n",
    "    ax.set(\n",
    "        xlabel=\"Prediction\",\n",
    "        ylabel=\"Actual\"\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff79a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:18:51.782350Z",
     "start_time": "2022-05-22T13:18:51.782350Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9075e76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-22T13:18:51.793698Z",
     "start_time": "2022-05-22T13:18:51.793698Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5e5357",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deep_learning] *",
   "language": "python",
   "name": "conda-env-deep_learning-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "252.533px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
